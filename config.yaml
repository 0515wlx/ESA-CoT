# Model configuration
model:
  hidden_size: 768
  num_attention_heads: 12
  num_hidden_layers: 6
  compress_dim: 128
  initial_token_len: 128
  local_token_len: 256
  top_k: 64
  vocab_size: 50257

# Dataset configuration
dataset:
  name: "THUDM/LongBench-v2"
  max_seq_len: 8192
  shuffle: true
  num_workers: 4

# Training configuration
training:
  batch_size: 8
  num_epochs: 10
  learning_rate: 5e-5
  warmup_steps: 1000
  save_dir: "./checkpoints"
  log_dir: "./logs"

# Optimizer configuration
optimizer:
  adam_epsilon: 1e-8
  weight_decay: 0.01
  max_grad_norm: 1.0

# Hardware configuration
hardware:
  device: "cuda"  # or "cpu"
  fp16: true
  gradient_accumulation_steps: 4